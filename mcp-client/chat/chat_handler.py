# Copyright (C) 2025 AIDC-AI
# This project is licensed under the MIT License (SPDX-License-identifier: MIT).

from datetime import timedelta
import json
import os
import time
import chainlit as cl
from typing import Any, Dict, List
from mcp import ClientSession
import re
from utils.llm_util import ModelInfo, ModelType

from litellm import acompletion
import litellm

from chat.starters import build_save_action
from utils.time_util import format_duration
from core.core import logger

save_starter_enabled = os.getenv("CHAINLIT_SAVE_STARTER_ENABLED", "false").lower() == "true"


def format_llm_error_message(model_name: str, error_str: str) -> str:
    """ç»Ÿä¸€çš„ LLM é”™è¯¯æ¶ˆæ¯æ ¼å¼åŒ–å‡½æ•°"""
    # å¤„ç†å¸¸è§çš„é”™è¯¯ç±»å‹ï¼Œæä¾›å‹å¥½çš„è‹±æ–‡é”™è¯¯ä¿¡æ¯
    if "RateLimitError" in error_str or "429" in error_str:
        if "quota" in error_str.lower() or "exceed" in error_str.lower():
            return f"âš ï¸ {model_name} API quota exceeded. Please check your plan and billing details."
        else:
            return f"âš ï¸ {model_name} API rate limit hit. Please try again later."
    elif "401" in error_str or "authentication" in error_str.lower():
        return f"ğŸ”‘ {model_name} API key is invalid. Please check your configuration."
    elif "403" in error_str or "permission" in error_str.lower():
        return f"ğŸš« {model_name} API access denied. Please check permissions."
    elif "timeout" in error_str.lower():
        return f"â° {model_name} API call timed out. Please retry."
    else:
        return f"âŒ {model_name} model call failed: {error_str}"


def get_all_tools() -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„ MCP å·¥å…·"""
    mcp_tools = cl.user_session.get("mcp_tools", {})
    all_tools = []
    for connection_tools in mcp_tools.values():
        all_tools.extend(connection_tools)
    return all_tools


def find_tool_connection(tool_name: str) -> str:
    """æŸ¥æ‰¾å·¥å…·æ‰€å±çš„ MCP è¿æ¥"""
    mcp_tools = cl.user_session.get("mcp_tools", {})
    for connection_name, tools in mcp_tools.items():
        if any(tool["function"]["name"] == tool_name for tool in tools):
            return connection_name
    return None


def _extract_content(content_list):
    """ä» CallToolResult çš„ content åˆ—è¡¨ä¸­æå–æ–‡æœ¬å†…å®¹"""
    if not content_list:
        return ""
    
    text_parts = []
    for content_item in content_list:
        # å¤„ç†ä¸åŒç±»å‹çš„å†…å®¹
        if hasattr(content_item, 'text'):
            # TextContent
            text_parts.append(content_item.text)
        elif hasattr(content_item, 'data'):
            # ImageContent æˆ–å…¶ä»–äºŒè¿›åˆ¶å†…å®¹
            text_parts.append(f"[Binary content: {getattr(content_item, 'mimeType', 'unknown')}]")
        elif hasattr(content_item, 'uri'):
            # EmbeddedResource
            text_parts.append(f"[Resource: {content_item.uri}]")
        else:
            # å…¶ä»–æœªçŸ¥ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            text_parts.append(str(content_item))
    
    return text_parts[0] if len(text_parts) == 1 else text_parts


@cl.step(type="tool")
async def execute_tool(tool_name: str, tool_input: Dict[str, Any]) -> str:
    """æ‰§è¡Œ MCP å·¥å…·è°ƒç”¨"""
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    def _format_result_with_duration(content: str) -> str:
        """ç»Ÿä¸€æ ¼å¼åŒ–å¸¦è€—æ—¶çš„ç»“æœ"""
        duration = time.time() - start_time
        return f"[Took {format_duration(duration)}] {content}"
    
    current_step = cl.context.current_step
    current_step.input = tool_input
    current_step.name = tool_name
    await current_step.update()
    
    def record_step():
        # è·å–ç°æœ‰çš„æ¶ˆæ¯å†å²
        current_steps = cl.user_session.get("current_steps", [])
        current_steps.append(current_step)
        # æ›´æ–°æ¶ˆæ¯å†å²
        cl.user_session.set("current_steps", current_steps)
    
    # æŸ¥æ‰¾å·¥å…·æ‰€å±çš„è¿æ¥
    mcp_name = find_tool_connection(tool_name)
    if not mcp_name:
        error_msg = json.dumps({"error": f"Tool {tool_name} not found in any MCP connection"})
        result_with_duration = _format_result_with_duration(error_msg)
        current_step.output = result_with_duration
        record_step()
        return result_with_duration
    
    # è·å– MCP ä¼šè¯
    mcp_session, _ = cl.context.session.mcp_sessions.get(mcp_name)
    if not mcp_session:
        error_msg = json.dumps({"error": f"MCP {mcp_name} not found in any MCP connection"})
        result_with_duration = _format_result_with_duration(error_msg)
        current_step.output = result_with_duration
        record_step()
        return result_with_duration
    
    try:
        # è°ƒç”¨ MCP å·¥å…·ï¼Œè¿”å› CallToolResult å¯¹è±¡
        logger.info(f"Calling MCP tool: {tool_name} with input: {tool_input}")
        result = await mcp_session.call_tool(tool_name, tool_input, read_timeout_seconds=timedelta(hours=1))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if result.isError:
            error_content = _extract_content(result.content)
            logger.error(f"Tool execution failed: {error_content}")
            error_msg = json.dumps({"error": f"Tool execution failed: {error_content}"})
            result_with_duration = _format_result_with_duration(error_msg)
            current_step.output = result_with_duration
            return result_with_duration
        
        # æå–å†…å®¹æ–‡æœ¬
        content = _extract_content(result.content)
        logger.info(f"Tool execution succeeded: {content}")
        result_with_duration = _format_result_with_duration(str(content))
        current_step.output = result_with_duration
        record_step()
        return result_with_duration
        
    except Exception as e:
        logger.error(f"Tool execution failed: {e}")
        error_msg = json.dumps({"error": str(e)})
        result_with_duration = _format_result_with_duration(error_msg)
        current_step.output = result_with_duration
        record_step()
        return result_with_duration


def _extract_and_clean_media_markers(text: str) -> tuple[Dict[str, List[str]], str]:
    """æå–åª’ä½“æ ‡è®°å¹¶æ¸…ç†æ–‡æœ¬
    
    Returns:
        tuple: (åª’ä½“æ–‡ä»¶å­—å…¸, æ¸…ç†åçš„æ–‡æœ¬)
               åª’ä½“æ–‡ä»¶å­—å…¸æ ¼å¼: {"images": [...], "audios": [...], "videos": [...]}
    """
    # åŒ¹é…ä¸åŒç±»å‹çš„åª’ä½“æ ‡è®°
    patterns = {
        "images": r'\[SHOW_IMAGE:([^\]]+)\]',
        "audios": r'\[SHOW_AUDIO:([^\]]+)\]',
        "videos": r'\[SHOW_VIDEO:([^\]]+)\]'
    }
    
    media_files = {"images": [], "audios": [], "videos": []}
    cleaned_text = text
    
    # æå–å„ç±»å‹åª’ä½“æ–‡ä»¶å¹¶æ¸…ç†æ–‡æœ¬
    for media_type, pattern in patterns.items():
        media_files[media_type] = re.findall(pattern, cleaned_text)
        cleaned_text = re.sub(pattern, '', cleaned_text)
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    cleaned_text = cleaned_text.rstrip()
    
    return media_files, cleaned_text


def _is_url(source: str) -> bool:
    """åˆ¤æ–­åª’ä½“æºæ˜¯å¦ä¸ºURL"""
    return source.startswith(('http://', 'https://'))


async def _process_media_markers(msg: cl.Message):
    """å¤„ç†æ¶ˆæ¯ä¸­çš„åª’ä½“æ ‡è®°"""
    if not msg.content:
        return
        
    # æå–åª’ä½“æ ‡è®°å¹¶æ¸…ç†æ–‡æœ¬
    media_files, cleaned_content = _extract_and_clean_media_markers(msg.content)
    
    # æ›´æ–°æ¶ˆæ¯å†…å®¹ï¼ˆç§»é™¤æ ‡è®°ï¼‰
    msg.content = cleaned_content
    
    # å¤„ç†å›¾ç‰‡
    for i, img_source in enumerate(media_files["images"]):
        img_source = img_source.strip()
        
        img_params = {
            "name": f"Generated_Image_{i+1}",
            "display": "inline",
            "size": "small",
        }
        
        if _is_url(img_source):
            img_params["url"] = img_source
        else:
            img_params["path"] = img_source
        
        img_element = cl.Image(**img_params)
        msg.elements.append(img_element)
        logger.info(f"Added image element: {img_source}")
    
    # å¤„ç†éŸ³é¢‘
    for i, audio_source in enumerate(media_files["audios"]):
        audio_source = audio_source.strip()
        
        audio_params = {
            "name": f"Generated_Audio_{i+1}",
            "display": "inline",
            "size": "small",
        }
        
        if _is_url(audio_source):
            audio_params["url"] = audio_source
        else:
            audio_params["path"] = audio_source
        
        audio_element = cl.Audio(**audio_params)
        msg.elements.append(audio_element)
        logger.info(f"Added audio element: {audio_source}")
    
    # å¤„ç†è§†é¢‘
    for i, video_source in enumerate(media_files["videos"]):
        video_source = video_source.strip()
        
        video_params = {
            "name": f"Generated_Video_{i+1}",
            "display": "inline",
            "size": "small",
        }
        
        if _is_url(video_source):
            video_params["url"] = video_source
        else:
            video_params["path"] = video_source
        
        video_element = cl.Video(**video_params)
        msg.elements.append(video_element)
        logger.info(f"Added video element: {video_source}")


async def _process_tool_call_delta(
    tool_calls_delta: List[Any], 
    current_tool_calls: Dict[int, Dict], 
    current_args: Dict[int, str]
):
    """å¤„ç†å·¥å…·è°ƒç”¨çš„å¢é‡æ•°æ®"""
    for tool_call_delta in tool_calls_delta:
        index = tool_call_delta.index
        
        # åˆå§‹åŒ–å·¥å…·è°ƒç”¨æ•°æ®ç»“æ„
        if index not in current_tool_calls:
            current_tool_calls[index] = {
                "id": "",
                "type": "function",
                "function": {"name": "", "arguments": ""}
            }
            current_args[index] = ""
        
        # ç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯
        if tool_call_delta.id:
            current_tool_calls[index]["id"] = tool_call_delta.id
        if tool_call_delta.function and tool_call_delta.function.name:
            current_tool_calls[index]["function"]["name"] = tool_call_delta.function.name
        if tool_call_delta.function and tool_call_delta.function.arguments:
            current_args[index] += tool_call_delta.function.arguments
            current_tool_calls[index]["function"]["arguments"] = current_args[index]


async def _execute_tool_calls(
    current_tool_calls: Dict[int, Dict], 
    messages: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨å¹¶æ›´æ–°æ¶ˆæ¯å†å²"""
    # æ„å»ºåŒ…å«å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯
    tool_calls_list = list(current_tool_calls.values())
    messages.append({
        "role": "assistant",
        "content": None,
        "tool_calls": tool_calls_list
    })
    
    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    for tool_call in tool_calls_list:
        tool_name = tool_call["function"]["name"]
        tool_args_str = tool_call["function"]["arguments"]
        
        try:
            # è§£æå·¥å…·å‚æ•°
            tool_args = json.loads(tool_args_str)
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_response = await execute_tool(tool_name, tool_args)
            
            # æ·»åŠ å·¥å…·å“åº”åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call["id"],
                "content": tool_response
            })
            
        except Exception as e:
            error_message = f"å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}"
            logger.error(error_message)
            # æ·»åŠ é”™è¯¯å“åº”åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call["id"],
                "content": error_message
            })
    
    return messages


async def _handle_stream_chunk(chunk, msg, current_tool_calls, current_args):
    """å¤„ç†æµå¼å“åº”çš„å•ä¸ªchunk"""
    choice = chunk.choices[0]
    delta = choice.delta
    has_tool_call = False
    
    # å¤„ç†å·¥å…·è°ƒç”¨
    if delta.tool_calls:
        has_tool_call = True
        await _process_tool_call_delta(
            delta.tool_calls, 
            current_tool_calls, 
            current_args
        )
    
    # å¤„ç†æ™®é€šæ–‡æœ¬å“åº”
    elif delta.content:
        await msg.stream_token(delta.content)
    
    return has_tool_call, choice.finish_reason


async def _handle_response(model_info, api_params, enhanced_messages, messages):
    """å¤„ç†æµå¼å“åº”"""
    # ä¸ºè¿™ä¸€è½®å“åº”åˆ›å»ºç‹¬ç«‹çš„æ¶ˆæ¯å¯¹è±¡
    msg = cl.Message(content="")
    
    current_tool_calls = {}
    current_args = {}
    has_tool_call = False
    
    try:
        # å‡†å¤‡ LiteLLM å‚æ•° - ç›´æ¥ä¼ é€’æ‰€æœ‰å¿…è¦å‚æ•°
        litellm_params = {
            "model": f"{model_info.provider}/{model_info.model}",
            "stream": True,
            "num_retries": 0,
            "timeout": 30,
            "api_key": model_info.api_key,
            "base_url": model_info.base_url or None,
            **api_params,
        }
        
        logger.info(f"Call LLM: {model_info.provider}/{model_info.model}")
        response = await acompletion(**litellm_params)
        
        try:
            async for chunk in response:
                chunk_has_tool_call, finish_reason = await _handle_stream_chunk(
                    chunk, msg, current_tool_calls, current_args
                )
                
                if chunk_has_tool_call:
                    has_tool_call = True
                
                # æ£€æŸ¥å®ŒæˆçŠ¶æ€
                if finish_reason == 'tool_calls':
                    try:
                        # å…ˆå‘é€å½“å‰è½®æ¬¡çš„æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰å†…å®¹çš„è¯ï¼‰
                        if msg.content and msg.content.strip():
                            await msg.send()
                        
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨
                        enhanced_messages = await _execute_tool_calls(
                            current_tool_calls, 
                            enhanced_messages
                        )
                        return enhanced_messages, True  # ç»§ç»­ä¸‹ä¸€è½®
                        
                    except Exception as e:
                        error_message = f"Error when processing tool calls: {str(e)}"
                        logger.error(error_message)
                        await msg.stream_token(f"\n{error_message}\n")
                        await msg.send()
                        return messages, False  # ç»“æŸå¤„ç†
                
                elif finish_reason:
                    # å…¶ä»–å®ŒæˆåŸå› ï¼Œç»“æŸæµå¼å¤„ç†
                    break
            
            # å¤„ç†åª’ä½“æ ‡è®°å¹¶å‘é€æ¶ˆæ¯
            if not has_tool_call:
                await _process_media_markers(msg)
                if msg.content and msg.content.strip():
                    if save_starter_enabled:
                        # åœ¨AIå›å¤æ¶ˆæ¯ä¸Šæ·»åŠ ä¿å­˜Action
                        msg.actions = [
                            build_save_action()
                        ]
                    await msg.send()
                
                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²è®°å½•
                if msg.content and msg.content.strip():
                    enhanced_messages.append({
                        "role": "assistant", 
                        "content": msg.content
                    })
            else:
                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å‘é€æ¶ˆæ¯ï¼ˆå·¥å…·è°ƒç”¨ç›¸å…³çš„æ¶ˆæ¯å·²ç»åœ¨åˆ«å¤„å¤„ç†ï¼‰
                await msg.send()
            
            return enhanced_messages, False  # ç»“æŸå¤„ç†
            
        except Exception as e:
            error_str = str(e)
            error_message = format_llm_error_message(model_info.name, error_str)
            logger.error(f"Stream processing error: {error_str}")
            await msg.stream_token(f"\n{error_message}\n")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦å¤„ç†åª’ä½“æ ‡è®°
            await _process_media_markers(msg)
            await msg.send()
            return messages, False
            
    except Exception as e:
        error_str = str(e)
        error_message = format_llm_error_message(model_info.name, error_str)
        logger.error(f"LiteLLM call failed: {error_str}")
        await msg.stream_token(f"\n{error_message}\n")
        # å³ä½¿å‡ºé”™ä¹Ÿè¦å¤„ç†åª’ä½“æ ‡è®°
        await _process_media_markers(msg)
        await msg.send()
        return messages, False


async def process_streaming_response(
    messages: List[Dict[str, Any]], 
    model_info: ModelInfo,
) -> List[Dict[str, Any]]:
    """
    å¤„ç†æµå¼å“åº”å’Œå·¥å…·è°ƒç”¨
    
    Args:
        messages: æ¶ˆæ¯å†å²
        model_info: ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
        
    Returns:
        æ›´æ–°åçš„æ¶ˆæ¯å†å²
    """
    
    # æ¸…ç†stepsï¼Œé’ˆå¯¹ç”¨æˆ·é‡æ–°ç¼–è¾‘å·²å‘é€æ¶ˆæ¯çš„åœºæ™¯åšå¤„ç†
    chat_messages = cl.chat_context.get()
    if len(chat_messages) >= 2:  # è‡³å°‘æœ‰2æ¡æ¶ˆæ¯æ‰éœ€è¦å¤„ç†
        # è·å–å€’æ•°ç¬¬äºŒæ¡æ¶ˆæ¯çš„æ—¶é—´æˆ³
        second_last_msg = chat_messages[-2]
        second_last_time = second_last_msg.created_at
        
        if second_last_time:
            # è·å–å½“å‰steps
            current_steps = cl.user_session.get("current_steps", [])
            # åªä¿ç•™æ—¶é—´æˆ³åœ¨å€’æ•°ç¬¬äºŒæ¡æ¶ˆæ¯ä¹‹å‰çš„steps
            filtered_steps = [
                step for step in current_steps 
                if str(step.created_at) <= second_last_time
            ]
            # æ›´æ–°steps
            cl.user_session.set("current_steps", filtered_steps)
    
    tools = get_all_tools()
    
    # æ³¨å…¥åª’ä½“æ˜¾ç¤ºç³»ç»ŸæŒ‡ä»¤
    enhanced_messages = messages.copy()
    
    while True:  # å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨
        # å‡†å¤‡ API å‚æ•°
        api_params = {
            "messages": enhanced_messages,
        }
        
        # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ å·¥å…·å‚æ•°
        if tools:
            api_params["tools"] = tools
            api_params["tool_choice"] = "auto"
        
        
        # æ‰€æœ‰å‚æ•°éƒ½é€šè¿‡ LiteLLM å‡½æ•°å‚æ•°ä¼ é€’ï¼Œä¸ä½¿ç”¨ç¯å¢ƒå˜é‡
        try:
            enhanced_messages, should_continue = await _handle_response(
                model_info, api_params, enhanced_messages, messages
            )
            
            if not should_continue:
                return enhanced_messages
                
        except Exception as e:
            error_str = str(e)
            error_message = format_llm_error_message(model_info.name, error_str)
            logger.error(f"LiteLLM main loop error: {error_str}")
            # å‘é€é”™è¯¯æ¶ˆæ¯ç»™ç”¨æˆ·
            error_msg = cl.Message(content=error_message)
            await error_msg.send()
            return enhanced_messages  # ç›´æ¥è¿”å›ï¼Œä¸è¦ç»§ç»­å¾ªç¯


# MCP è¿æ¥ç®¡ç†çš„ä¾¿æ·å‡½æ•°
async def handle_mcp_connect(connection, session: ClientSession, tools_converter_func):
    """å¤„ç† MCP è¿æ¥çš„é€šç”¨é€»è¾‘"""
    tools_result = await session.list_tools()
    openai_tools = tools_converter_func(tools_result.tools)
    
    mcp_tools = cl.user_session.get("mcp_tools", {})
    mcp_tools[connection.name] = openai_tools
    cl.user_session.set("mcp_tools", mcp_tools)

async def handle_mcp_disconnect(name: str):
    """å¤„ç† MCP æ–­å¼€è¿æ¥çš„é€šç”¨é€»è¾‘"""
    mcp_tools = cl.user_session.get("mcp_tools", {})
    if name in mcp_tools:
        del mcp_tools[name]
    cl.user_session.set("mcp_tools", mcp_tools) 

 